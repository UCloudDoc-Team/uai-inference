{{indexmenu_n>1}}

# UAI-Inference基础知识
AI 在线服务UAI-Inference （UCloud AI online Inference）是面向AI在线Inference服务的大规模分布式计算平台。可提供数万的AI在线服务节点，系统自动完成AI请求的负载均衡，并自动实行节点动态扩容和回收并按实际使用量计费。

**UAI-Inference架构示意图**

{{:ai:uai-service:intro:ai_inference产品示意图.png|}}

# UAI-Inference 服务模式
UAI-Inference 会帮助用户自动部署分布式AI在线服务，系统提供统一的请求访问入口，并自动完成AI请求的负载均衡，同时提供监控、灰度发布、日志等辅助功能。用户可以轻松地通过HTTP请求访问部署在UAI-Inference平台上的AI服务。
目前UAI-Inference平台提供两种算力模式：**独占模式**和**共享模式**。

### 算力独占模式
算力独占模式下，用户所使用的在线服务计算节点的所有计算资源都是有保障的（CPU、GPU、内存），例如GPU计算节点中，用户是独享单块GPU卡的所有算力。用户可以通过控制台或者API动态控制计算节点的数量，以达到弹性的目的。独占模式下，我们以用户实际所占用的节点的使用时间总和来结算费用，结算单位精确到分钟，详细请查看[[ai:uai-inference:price]]

算力独占模式目前已经支持用户直接部署公网可直接访问的AI在线服务，同时为了防止公网服务被恶意滥用，我们提供token令牌鉴权的方式来控制服务的访问权限。为APP进行授权的方式请查看[[ai:uai-inference:use:auth]]

### 算力共享模式
算力共享模式下，用户所使用的在线服务计算节点的计算资源是海量CPU计算资源池，该计算资源池中的单个计算节点的算力存在被抢占的可能，但是整个资源池的算力资源数量有充分保障。用户部署的AI在线服务的计算节点由系统自动实行动态扩容和回收并按实际使用量计费。共享模式下，我们根据推理请求计算所消耗的实际算力资源来结算费用，详细请查看[[ai:uai-inference:price]]

目前算力共享模式不支持用户直接部署公网可直接访问的AI在线服务，用户只能通过内网访问部署的AI在线服务，同时也无需使用token令牌鉴权的请求方式。

## UAI-Inference 主要模块
UAI-Inference包括两个模块：

+ 用户态SDK工具包：SDK工具包包含了inference代码框架；代码以及数据打包模板；第三方依赖库描述模板。使用过程中，1您只需要根据SDK工具包内的代码框架编写inference代码；2准备好代码和模型；3准备好基础镜像，就可以通过打包工具一键完成任务部署。

+ AI在线服务PaaS平台，AI在线服务PaaS平台为分布式PaaS平台，可以同时管理上千计算节点，每个计算节点都是同构的节点具有相等的计算能力。该平台拥有自动请求负载均衡、自动资源管理的功能。您只需要将业务部署在AI在线服务PaaS平台上，就不用再关心后续运维的问题。

**何时使用AI在线服务？**

UAI-Inference是一款面向AI在线Fnference服务的平台，您可以将您AI业务中在线服务的模块快速部署于UAI-Inference上，通过利用UCloud云计算平台强大的计算能力来应对小到每小时几十个请求，大到每秒数千次请求的负载。UAI-Inference平台会根据负载完成自动缩放，您完全无需担心计算资源不足或浪费的问题。

UAI-Inference平台普遍适用于常见的AI在线服务场景，如图像识别、自然语言处理等等。只需要这些AI服务在UAI-Inference的节点上运行满足延时要求，就可以享受UAI-Inference便捷的服务。

**AI在线服务任务部署流程图**

{{:ai:uai-service:intro:ai_inference部署.png|}}

